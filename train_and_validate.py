# -*- coding: utf-8 -*-
"""XGBoost Ranker + Rule-based Rerank

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/sakethbaddam/xgboost-ranker-rule-based-rerank.7a24c721-2245-49ae-aa43-0994f5253753.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250729/auto/storage/goog4_request%26X-Goog-Date%3D20250729T070035Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4b4dcffb0180a5f886e90cd8dd2215e4e6ad5a86026c166f39f80641778b84c47f8a52758f1a558f33f5205d3e5065f9b7ea804c92e2f9c65e62c2371fd17f90e34a94b040d2e5536e779bd9acd2ccad785e90facd030e44716b6e5db6f398c3fabe4e93cd3277ba66c819f163c69029e77f1218912f0e338d9255f4ae930bd91898c5fe7a5755d2f0829c6027e906f7a2a511f9f9aaa9687d5b7adcc323a1011f5e8b5ac8aa3d63e1024b1c8d63a4c607814f3f12c79df20b0d59c1cb0dd29118176779496c8a28bf9107ea0f450bb31feed799d12bee6d355a7d47028716a4246296fcdf1667ca8d4a1399a0aec95b9530223b60def9cb128c05abc90424ba
"""

# FlightRank 2025 - Local Training Script
print('Starting FlightRank 2025 training pipeline...')

"""# AeroClub RecSys 2025 - XGBoost Ranking Baseline

This notebook implements an improved ranking approach using XGBoost and Polars for the AeroClub recommendation challenge.

# <div  style="text-align:center;padding:10.0px; background:#000000"> Thank you for your attention! Please upvote if you like it) </div>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -U xgboost
# !pip install -U polars

import polars as pl
import numpy as np
import matplotlib.pyplot as plt
import time
import xgboost as xgb
from sklearn.model_selection import GroupKFold
import pickle
import os

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

# Load data from local files
print('Loading data files...')
train = pl.read_parquet('./data/train.parquet')
test = pl.read_parquet('./data/test.parquet').with_columns(pl.lit(0, dtype=pl.Int64).alias("selected"))

# Drop __index_level_0__ column if it exists
if '__index_level_0__' in train.columns:
    train = train.drop('__index_level_0__')
if '__index_level_0__' in test.columns:
    test = test.drop('__index_level_0__')

print(f'Train shape: {train.shape}')
print(f'Test shape: {test.shape}')
print('Data loading complete.')

data_raw = pl.concat((train, test))

"""## Helpers"""

def hitrate_at_3(y_true, y_pred, groups):
    df = pl.DataFrame({
        'group': groups,
        'pred': y_pred,
        'true': y_true
    })

    return (
        df.filter(pl.col("group").count().over("group") > 10)
        .sort(["group", "pred"], descending=[False, True])
        .group_by("group", maintain_order=True)
        .head(3)
        .group_by("group")
        .agg(pl.col("true").max())
        .select(pl.col("true").mean())
        .item()
    )

"""## Feature Engineering"""

df = data_raw.clone()

# More efficient duration to minutes converter
def dur_to_min(col):
    # Extract days and time parts in one pass
    days = col.str.extract(r"^(\d+)\.", 1).cast(pl.Int64).fill_null(0) * 1440
    time_str = pl.when(col.str.contains(r"^\d+\.")).then(col.str.replace(r"^\d+\.", "")).otherwise(col)
    hours = time_str.str.extract(r"^(\d+):", 1).cast(pl.Int64).fill_null(0) * 60
    minutes = time_str.str.extract(r":(\d+):", 1).cast(pl.Int64).fill_null(0)
    return (days + hours + minutes).fill_null(0)

# Process duration columns
dur_cols = ["legs0_duration", "legs1_duration"] + [f"legs{l}_segments{s}_duration" for l in (0, 1) for s in (0, 1)]
dur_exprs = [dur_to_min(pl.col(c)).alias(c) for c in dur_cols if c in df.columns]

# Apply duration transformations first
if dur_exprs:
    df = df.with_columns(dur_exprs)

# Precompute marketing carrier columns check
mc_cols = [f'legs{l}_segments{s}_marketingCarrier_code' for l in (0, 1) for s in range(4)]
mc_exists = [col for col in mc_cols if col in df.columns]

# Combine all initial transformations
df = df.with_columns([
        # Price features
        # (pl.col("totalPrice") / (pl.col("taxes") + 1)).alias("price_per_tax"),
        (pl.col("taxes") / (pl.col("totalPrice") + 1)).alias("tax_rate"),
        pl.col("totalPrice").log1p().alias("log_price"),

        # Duration features
        (pl.col("legs0_duration").fill_null(0) + pl.col("legs1_duration").fill_null(0)).alias("total_duration"),
        pl.when(pl.col("legs1_duration").fill_null(0) > 0)
            .then(pl.col("legs0_duration") / (pl.col("legs1_duration") + 1))
            .otherwise(1.0).alias("duration_ratio"),

        # Trip type
        (pl.col("legs1_duration").is_null() |
         (pl.col("legs1_duration") == 0) |
         pl.col("legs1_segments0_departureFrom_airport_iata").is_null()).cast(pl.Int32).alias("is_one_way"),

        # Total segments count
        (pl.sum_horizontal(pl.col(col).is_not_null().cast(pl.UInt8) for col in mc_exists)
         if mc_exists else pl.lit(0)).alias("l0_seg"),

        # FF features
        (pl.col("frequentFlyer").fill_null("").str.count_matches("/") +
         (pl.col("frequentFlyer").fill_null("") != "").cast(pl.Int32)).alias("n_ff_programs"),

        # Binary features
        pl.col("corporateTariffCode").is_not_null().cast(pl.Int32).alias("has_corporate_tariff"),
        (pl.col("pricingInfo_isAccessTP") == 1).cast(pl.Int32).alias("has_access_tp"),

        # Baggage & fees
        # (pl.col("legs0_segments0_baggageAllowance_quantity").fill_null(0) +
        #  pl.col("legs1_segments0_baggageAllowance_quantity").fill_null(0)).alias("baggage_total"),
        # (pl.col("miniRules0_monetaryAmount").fill_null(0) +
        #  pl.col("miniRules1_monetaryAmount").fill_null(0)).alias("total_fees"),

        (
            (pl.col("miniRules0_monetaryAmount") == 0)
            & (pl.col("miniRules0_statusInfos") == 1)
        )
        .cast(pl.Int8)
        .alias("free_cancel"),
        (
            (pl.col("miniRules1_monetaryAmount") == 0)
            & (pl.col("miniRules1_statusInfos") == 1)
        )
        .cast(pl.Int8)
        .alias("free_exchange"),

        # Routes & carriers
        pl.col("searchRoute").is_in(["MOWLED/LEDMOW", "LEDMOW/MOWLED", "MOWLED", "LEDMOW"])
            .cast(pl.Int32).alias("is_popular_route"),

        # Cabin
        pl.mean_horizontal(["legs0_segments0_cabinClass", "legs1_segments0_cabinClass"]).alias("avg_cabin_class"),
        (pl.col("legs0_segments0_cabinClass").fill_null(0) -
         pl.col("legs1_segments0_cabinClass").fill_null(0)).alias("cabin_class_diff"),
])

# Segment counts - more efficient
seg_exprs = []
for leg in (0, 1):
    seg_cols = [f"legs{leg}_segments{s}_duration" for s in range(4) if f"legs{leg}_segments{s}_duration" in df.columns]
    if seg_cols:
        seg_exprs.append(
            pl.sum_horizontal(pl.col(c).is_not_null() for c in seg_cols)
                .cast(pl.Int32).alias(f"n_segments_leg{leg}")
        )
    else:
        seg_exprs.append(pl.lit(0).cast(pl.Int32).alias(f"n_segments_leg{leg}"))

# Add segment-based features
# First create segment counts
df = df.with_columns(seg_exprs)

# Then use them for derived features
df = df.with_columns([
    (pl.col("n_segments_leg0") + pl.col("n_segments_leg1")).alias("total_segments"),
    (pl.col("n_segments_leg0") == 1).cast(pl.Int32).alias("is_direct_leg0"),
    pl.when(pl.col("is_one_way") == 1).then(0)
        .otherwise((pl.col("n_segments_leg1") == 1).cast(pl.Int32)).alias("is_direct_leg1"),
])

# More derived features
df = df.with_columns([
    (pl.col("is_direct_leg0") & pl.col("is_direct_leg1")).cast(pl.Int32).alias("both_direct"),
    ((pl.col("isVip") == 1) | (pl.col("n_ff_programs") > 0)).cast(pl.Int32).alias("is_vip_freq"),
    # (pl.col("baggage_total") > 0).cast(pl.Int32).alias("has_baggage"),
    # (pl.col("total_fees") > 0).cast(pl.Int32).alias("has_fees"),
    # (pl.col("total_fees") / (pl.col("totalPrice") + 1)).alias("fee_rate"),
    pl.col("Id").count().over("ranker_id").alias("group_size"),
])

# Add major carrier flag if column exists
if "legs0_segments0_marketingCarrier_code" in df.columns:
    df = df.with_columns(
        pl.col("legs0_segments0_marketingCarrier_code").is_in(["SU", "S7"])
            .cast(pl.Int32).alias("is_major_carrier")
    )
else:
    df = df.with_columns(pl.lit(0).alias("is_major_carrier"))

df = df.with_columns(pl.col("group_size").log1p().alias("group_size_log"))

# Time features - batch process
time_exprs = []
for col in ("legs0_departureAt", "legs0_arrivalAt", "legs1_departureAt", "legs1_arrivalAt"):
    if col in df.columns:
        dt = pl.col(col).str.to_datetime(strict=False)
        h = dt.dt.hour().fill_null(12)
        time_exprs.extend([
            h.alias(f"{col}_hour"),
            dt.dt.weekday().fill_null(0).alias(f"{col}_weekday"),
            (((h >= 6) & (h <= 9)) | ((h >= 17) & (h <= 20))).cast(pl.Int32).alias(f"{col}_business_time")
        ])
if time_exprs:
    df = df.with_columns(time_exprs)

# Batch rank computations - more efficient with single pass
# First apply the columns that will be used for ranking
df = df.with_columns([
    pl.col("group_size").log1p().alias("group_size_log"),
])

# Price and duration basic ranks
rank_exprs = []
for col, alias in [("totalPrice", "price"), ("total_duration", "duration")]:
    rank_exprs.append(pl.col(col).rank().over("ranker_id").alias(f"{alias}_rank"))

# Price-specific features
price_exprs = [
    (pl.col("totalPrice").rank("average").over("ranker_id") /
     pl.col("totalPrice").count().over("ranker_id")).alias("price_pct_rank"),
    (pl.col("totalPrice") == pl.col("totalPrice").min().over("ranker_id")).cast(pl.Int32).alias("is_cheapest"),
    ((pl.col("totalPrice") - pl.col("totalPrice").median().over("ranker_id")) /
     (pl.col("totalPrice").std().over("ranker_id") + 1)).alias("price_from_median"),
    (pl.col("l0_seg") == pl.col("l0_seg").min().over("ranker_id")).cast(pl.Int32).alias("is_min_segments"),
]

# Apply initial ranks
df = df.with_columns(rank_exprs + price_exprs)

# Cheapest direct - more efficient
direct_cheapest = (
    df.filter(pl.col("is_direct_leg0") == 1)
    .group_by("ranker_id")
    .agg(pl.col("totalPrice").min().alias("min_direct"))
)

df = df.join(direct_cheapest, on="ranker_id", how="left").with_columns(
    ((pl.col("is_direct_leg0") == 1) &
     (pl.col("totalPrice") == pl.col("min_direct"))).cast(pl.Int32).fill_null(0).alias("is_direct_cheapest")
).drop("min_direct")

# Popularity features - efficient join
df = (
    df.join(
        train.group_by('legs0_segments0_marketingCarrier_code').agg(pl.mean('selected').alias('carrier0_pop')),
        on='legs0_segments0_marketingCarrier_code',
        how='left'
    )
    .join(
        train.group_by('legs1_segments0_marketingCarrier_code').agg(pl.mean('selected').alias('carrier1_pop')),
        on='legs1_segments0_marketingCarrier_code',
        how='left'
    )
    .with_columns([
        pl.col('carrier0_pop').fill_null(0.0),
        pl.col('carrier1_pop').fill_null(0.0),
    ])
)

# Final features including popularity
df = df.with_columns([
    (pl.col('carrier0_pop') * pl.col('carrier1_pop')).alias('carrier_pop_product'),
])

# Group-context gap features for HitRate@3 optimization
df = df.with_columns([
    # 1. price_gap: flight price minus minimum price in its ranker_id group
    (pl.col("totalPrice") - pl.col("totalPrice").min().over("ranker_id")).alias("price_gap"),
    
    # 2. departure_gap: flight departure time minus minimum departure time in group
    # Using legs0_departureAt_hour for departure time context (morning departures are often preferred)
    (pl.col("legs0_departureAt_hour") - pl.col("legs0_departureAt_hour").min().over("ranker_id")).alias("departure_gap").fill_null(0),
    
    # 3. is_best_direct: Binary flag for flights that are both cheapest AND direct among direct flights in group
    # First, identify the minimum price among direct flights in each group
    pl.when(pl.col("is_direct_leg0") == 1)
        .then(pl.col("totalPrice"))
        .otherwise(None)
        .min()
        .over("ranker_id")
        .alias("min_direct_price"),
]).with_columns([
    # Then create the is_best_direct flag: direct flight AND matches minimum price among direct flights
    ((pl.col("is_direct_leg0") == 1) & 
     (pl.col("totalPrice") == pl.col("min_direct_price")) &
     pl.col("min_direct_price").is_not_null())
    .cast(pl.Int32)
    .fill_null(0)
    .alias("is_best_direct")
]).drop("min_direct_price")  # Clean up temporary column

# Fill nulls
data = df.with_columns(
    [pl.col(c).fill_null(0) for c in df.select(pl.selectors.numeric()).columns] +
    [pl.col(c).fill_null("missing") for c in df.select(pl.selectors.string()).columns]
)

"""## Feature Selection"""

# Categorical features
cat_features = [
    'nationality', 'searchRoute', 'corporateTariffCode',
    'bySelf', 'sex', 'companyID',
    # Leg 0 segments 0-1
    'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata',
    'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_departureFrom_airport_iata',
    'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code',
    'legs0_segments0_flightNumber',
    'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata',
    'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_departureFrom_airport_iata',
    'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code',
    'legs0_segments1_flightNumber',
    # Leg 1 segments 0-1
    'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata',
    'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_departureFrom_airport_iata',
    'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code',
    'legs1_segments0_flightNumber',
    'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata',
    'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_departureFrom_airport_iata',
    'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code',
    'legs1_segments1_flightNumber',
]

# Columns to exclude (uninformative or problematic)
exclude_cols = [
    'Id', 'ranker_id', 'selected', 'profileId', 'requestDate',
    'legs0_departureAt', 'legs0_arrivalAt', 'legs1_departureAt', 'legs1_arrivalAt',
    'miniRules0_percentage', 'miniRules1_percentage',  # >90% missing
    'frequentFlyer',  # Already processed
    # Exclude constant columns
    'pricingInfo_passengerCount'
]

for leg in [0, 1]:
    for seg in [0, 1]:
        if seg == 0:
            suffixes = [
                "seatsAvailable",
            ]
        else:
            suffixes = [
                "cabinClass",
                "seatsAvailable",
                "baggageAllowance_quantity",
                "baggageAllowance_weightMeasurementType",
                "aircraft_code",
                "arrivalTo_airport_city_iata",
                "arrivalTo_airport_iata",
                "departureFrom_airport_iata",
                "flightNumber",
                "marketingCarrier_code",
                "operatingCarrier_code",
            ]
        for suffix in suffixes:
            exclude_cols.append(f"legs{leg}_segments{seg}_{suffix}")


# Exclude segment 2-3 columns (>98% missing)
for leg in [0, 1]:
    for seg in [2, 3]:
        for suffix in ['aircraft_code', 'arrivalTo_airport_city_iata', 'arrivalTo_airport_iata',
                      'baggageAllowance_quantity', 'baggageAllowance_weightMeasurementType',
                      'cabinClass', 'departureFrom_airport_iata', 'duration', 'flightNumber',
                      'marketingCarrier_code', 'operatingCarrier_code', 'seatsAvailable']:
            exclude_cols.append(f'legs{leg}_segments{seg}_{suffix}')

feature_cols = [col for col in data.columns if col not in exclude_cols]
cat_features_final = [col for col in cat_features if col in feature_cols]

print(f"Using {len(feature_cols)} features ({len(cat_features_final)} categorical)")

X = data.select(feature_cols)
y = data.select('selected')
groups = data.select('ranker_id')

"""## Robust Validation Framework - GroupKFold Cross-Validation"""

print("\nðŸ”§ CRITICAL VALIDATION FIX: Implementing GroupKFold Cross-Validation")
print("Previous validation was severely flawed (0.930 local vs 0.497 public)")
print("New approach: 5-fold GroupKFold on ranker_id to prevent data leakage\n")

# Prepare training data only (separate test for final predictions)
n_train = train.height
X_train = X[:n_train]
y_train = y[:n_train]
groups_train = groups[:n_train]
X_test = X[n_train:]
y_test = y[n_train:]
groups_test = groups[n_train:]

# Encode categorical features for XGBoost
data_xgb_train = X_train.with_columns([(pl.col(c).rank("dense") - 1).fill_null(-1).cast(pl.Int16) for c in cat_features_final])
data_xgb_test = X_test.with_columns([(pl.col(c).rank("dense") - 1).fill_null(-1).cast(pl.Int16) for c in cat_features_final])

# Convert to numpy for sklearn compatibility
X_train_np = data_xgb_train.to_numpy()
y_train_np = y_train.to_numpy().flatten()
groups_train_np = groups_train.to_numpy().flatten()

# 5-Fold GroupKFold Cross-Validation
print("Starting 5-fold GroupKFold cross-validation...")
gkf = GroupKFold(n_splits=5)
fold_scores = []
best_score = 0
best_model = None

for fold_idx, (train_idx, val_idx) in enumerate(gkf.split(X_train_np, y_train_np, groups_train_np), 1):
    print(f"\n--- Fold {fold_idx}/5 ---")
    
    # Split data for this fold
    X_fold_train, X_fold_val = X_train_np[train_idx], X_train_np[val_idx]
    y_fold_train, y_fold_val = y_train_np[train_idx], y_train_np[val_idx]
    groups_fold_train, groups_fold_val = groups_train_np[train_idx], groups_train_np[val_idx]
    
    # Calculate group sizes for XGBoost ranking
    unique_groups_train, group_sizes_train = np.unique(groups_fold_train, return_counts=True)
    unique_groups_val, group_sizes_val = np.unique(groups_fold_val, return_counts=True)
    
    # Sort by group to match XGBoost expectations
    train_sort_idx = np.argsort(groups_fold_train)
    val_sort_idx = np.argsort(groups_fold_val)
    
    X_fold_train_sorted = X_fold_train[train_sort_idx]
    y_fold_train_sorted = y_fold_train[train_sort_idx]
    X_fold_val_sorted = X_fold_val[val_sort_idx]
    y_fold_val_sorted = y_fold_val[val_sort_idx]
    groups_fold_val_sorted = groups_fold_val[val_sort_idx]
    
    # Create XGBoost DMatrix
    dtrain_fold = xgb.DMatrix(X_fold_train_sorted, label=y_fold_train_sorted, group=group_sizes_train, feature_names=data_xgb_train.columns)
    dval_fold = xgb.DMatrix(X_fold_val_sorted, label=y_fold_val_sorted, group=group_sizes_val, feature_names=data_xgb_train.columns)
    
    # XGBoost parameters (same as original)
    xgb_params = {
        'objective': 'rank:pairwise',
        'eval_metric': 'ndcg@3',
        "learning_rate": 0.022641389657079056,
        "max_depth": 14,
        "min_child_weight": 2,
        "subsample": 0.8842234913702768,
        "colsample_bytree": 0.45840689146263086,
        "gamma": 3.3084297630544888,
        "lambda": 6.952586917313028,
        "alpha": 0.6395254133055179,
        'seed': RANDOM_STATE,
        'n_jobs': -1,
    }
    
    # Train model for this fold
    fold_model = xgb.train(
        xgb_params,
        dtrain_fold,
        num_boost_round=800,
        evals=[(dtrain_fold, 'train'), (dval_fold, 'val')],
        verbose_eval=0  # Reduce output for cleaner logs
    )
    
    # Evaluate on validation set
    val_predictions = fold_model.predict(dval_fold)
    
    # Calculate HitRate@3 with proper group filtering (>10 options)
    fold_hr3 = hitrate_at_3(y_fold_val_sorted, val_predictions, groups_fold_val_sorted)
    fold_scores.append(fold_hr3)
    
    print(f"Fold {fold_idx} HitRate@3: {fold_hr3:.5f}")
    
    # Track best model
    if fold_hr3 > best_score:
        best_score = fold_hr3
        best_model = fold_model

# Report cross-validation results
avg_score = np.mean(fold_scores)
std_score = np.std(fold_scores)
print(f"\nðŸŽ¯ VALIDATION RESULTS:")
print(f"Average HitRate@3: {avg_score:.5f} (+/- {std_score:.5f})")
print(f"Individual fold scores: {[f'{score:.5f}' for score in fold_scores]}")
print(f"Best single fold: {best_score:.5f}")

if avg_score < 0.8:  # Much more realistic
    print("âœ… SUCCESS: Validation score is now realistic (no more data leakage!)")
else:
    print("âš ï¸  WARNING: Score still seems too high - check for remaining leakage")

# XGBoost parameters
xgb_params = {
    'objective': 'rank:pairwise',
    'eval_metric': 'ndcg@3',
    "learning_rate": 0.022641389657079056,
    "max_depth": 14,
    "min_child_weight": 2,
    "subsample": 0.8842234913702768,
    "colsample_bytree": 0.45840689146263086,
    "gamma": 3.3084297630544888,
    "lambda": 6.952586917313028,
    "alpha": 0.6395254133055179,
    'seed': RANDOM_STATE,
    'n_jobs': -1,
    # 'device': 'cuda'
}

# Train final model on full training data
print("\nðŸ—ï¸  Training final model on full training data...")

# Prepare full training data
X_full_train_sorted_idx = np.argsort(groups_train_np)
X_full_train_sorted = X_train_np[X_full_train_sorted_idx]
y_full_train_sorted = y_train_np[X_full_train_sorted_idx]

unique_groups_full, group_sizes_full = np.unique(groups_train_np, return_counts=True)
dtrain_full = xgb.DMatrix(X_full_train_sorted, label=y_full_train_sorted, group=group_sizes_full, feature_names=data_xgb_train.columns)

# Train final model
final_model = xgb.train(
    xgb_params,
    dtrain_full,
    num_boost_round=800,
    verbose_eval=50
)

# Save the trained model
os.makedirs('./models', exist_ok=True)
with open('./models/best_model.pkl', 'wb') as f:
    pickle.dump(final_model, f)
    
print("âœ… Model saved to ./models/best_model.pkl")

# Feature importance from final model
final_importance = final_model.get_score(importance_type='gain')
final_importance_df = pl.DataFrame(
    [{'feature': k, 'importance': v} for k, v in final_importance.items()]
).sort('importance', descending=bool(1))
print("\nðŸ“Š Top 20 Feature Importances:")
print(final_importance_df.head(20).to_pandas().to_string())

"""## Submission"""

def re_rank(test: pl.DataFrame, submission_xgb: pl.DataFrame, penalty_factor=0.1):
    COLS_TO_COMPARE = [
        "legs0_departureAt",
        "legs0_arrivalAt",
        "legs1_departureAt",
        "legs1_arrivalAt",
        "legs0_segments0_flightNumber",
        "legs1_segments0_flightNumber",
        "legs0_segments0_aircraft_code",
        "legs1_segments0_aircraft_code",
        "legs0_segments0_departureFrom_airport_iata",
        "legs1_segments0_departureFrom_airport_iata",
    ]

    test = test.with_columns(
        [pl.col(c).cast(str).fill_null("NULL") for c in COLS_TO_COMPARE]
    )

    df = submission_xgb.join(test, on=["Id", "ranker_id"], how="left")

    df = df.with_columns(
        (
            pl.col("legs0_departureAt")
            + "_"
            + pl.col("legs0_arrivalAt")
            + "_"
            + pl.col("legs1_departureAt")
            + "_"
            + pl.col("legs1_arrivalAt")
            + "_"
            + pl.col("legs0_segments0_flightNumber")
            + "_"
            + pl.col("legs1_segments0_flightNumber")
        ).alias("flight_hash")
    )

    df = df.with_columns(
        pl.max("pred_score")
        .over(["ranker_id", "flight_hash"])
        .alias("max_score_same_flight")
    )

    df = df.with_columns(
        (
            pl.col("pred_score")
            - penalty_factor * (pl.col("max_score_same_flight") - pl.col("pred_score"))
        ).alias("reorder_score")
    )

    df = df.with_columns(
        pl.col("reorder_score")
        .rank(method="ordinal", descending=True)
        .over("ranker_id")
        .cast(pl.Int32)
        .alias("new_selected")
    )

    return df.select(["Id", "ranker_id", "new_selected", "pred_score", "reorder_score"])

# Generate test predictions using final model
unique_groups_test, group_sizes_test = np.unique(groups_test.to_numpy().flatten(), return_counts=True)
test_sort_idx = np.argsort(groups_test.to_numpy().flatten())
X_test_sorted = data_xgb_test.to_numpy()[test_sort_idx]
dtest_final = xgb.DMatrix(X_test_sorted, group=group_sizes_test, feature_names=data_xgb_test.columns)

test_predictions = final_model.predict(dtest_final)

# Restore original order
reverse_sort_idx = np.argsort(test_sort_idx)
test_predictions_original_order = test_predictions[reverse_sort_idx]

submission_xgb = (
    test.select(['Id', 'ranker_id'])
    .with_columns(pl.Series('pred_score', test_predictions_original_order))
    .with_columns(
        pl.col('pred_score')
        .rank(method='ordinal', descending=True)
        .over('ranker_id')
        .cast(pl.Int32)
        .alias('selected')
    )
    .select(['Id', 'ranker_id', 'selected', 'pred_score'])
)

top = re_rank(test, submission_xgb)

submission_xgb = (
    submission_xgb.join(top, on=["Id", "ranker_id"], how="left")
    .with_columns(
        [
            pl.when(pl.col("new_selected").is_not_null())
            .then(pl.col("new_selected"))
            .otherwise(pl.col("selected"))
            .alias("selected")
        ]
    )
    .select(["Id", "ranker_id", "selected"])
)


submission_xgb.write_csv('submission.csv')

